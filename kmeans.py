# -*- coding: utf-8 -*-
"""kmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NurHKi8cSDo67-Kws1WJjXyeoIiU4JW-
"""

from __future__ import absolute_import, unicode_literals

from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
import nltk
import pandas as pd
from sklearn.cluster import KMeans

import json

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"This segment contains functions to generate word list"
punctuation = u",.?!()-_\"\’\\\n\r\t;:+*<>@#§^$%&|/"

stop_words_eng = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
tag_dict = {"J": wn.ADJ,
            "N": wn.NOUN,
            "V": wn.VERB,
            "R": wn.ADV}

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
libdir = "/content/drive/MyDrive/Colab\ Notebooks/"
# %run {libdir}custom_stopwords.ipynb

def extract_wnpostag_from_postag(tag):
	#take the first letter of the tag
  #the second parameter is an "optional" in case of missing key in the dictionary 
  return tag_dict.get(tag[0].upper(), None)

def lemmatize_tupla_word_postag(tupla):
  """
  giving a tupla of the form (wordString, posTagString) like ('guitar', 'NN'), return the lemmatized word
  """
  tag = extract_wnpostag_from_postag(tupla[1])
   
  return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]
  
def is_alpha(word):
  if "-" in word:
    for x in word.split(sep="-"):
      if is_alpha(x):
      	return True
      else:
      	return False
    return True
  elif word.isalpha():
    return True
  else:
    return False

def bag_of_words(sentence, stop_words=None):
    if stop_words is None:
        stop_words = stop_words_eng
    original_words = word_tokenize(sentence)
    original_words = [words.lower() for words in original_words]
    original_words = [word for word in original_words if is_alpha(word)]
    
    cleaned_words = [ w for w in original_words if (w not in stop_words) ]
    
    tagged_words = nltk.pos_tag(cleaned_words) #returns a list of tuples: (word, tagString) like ('And', 'CC')
    #original_words = None
    tagged_words = [tagged_word for tagged_word in tagged_words if (tagged_word[1][0] == 'N')]
    
    lemmatized_words = [lemmatize_tupla_word_postag(ow) for ow in tagged_words ]
    #tagged_words = None
    cleaned_words = [ w for w in lemmatized_words if (w not in stop_words) ]
    #lemmatized_words = None
    return cleaned_words

#----------------------------------------------------------------------------

def kmeans_rmodel(txt, glove_model, k = 15):
	try:
		sentences = sent_tokenize(txt)
		words = []

		wrong_words = ['market', 'segment', 'research', 'report', 'analysis', 'company', 'cookies', 'table']
		 
		for sentence in sentences: 
		    ww = False 
		    for w_word in wrong_words: 
		        if w_word in sentence.lower(): 
		            ww = True 
		    if not(ww): 
		        words.extend(bag_of_words(sentence, stop_words = stop_words))


		not_in_vocab = []
		df = pd.DataFrame()

		counter = 0
		df_dic = {}
		for word in words:
		    try:
		        df[counter] = glove_model[word]
		        counter += 1
		        df_dic[counter] = word
		    except:
		        not_in_vocab.append(word)
		        continue
		df = df.T

		output = {}

		kmeans = KMeans(n_clusters=k).fit(df)  
		print('Ran KMeans ', k , ' successfully')
		for i in range(k):
			v = kmeans.cluster_centers_[i]
			keywords = [keyword[0] for keyword in glove_model.most_similar(positive=[v], topn=5)]
			df_kw = pd.DataFrame()
			counter = 0 
			for keyword in keywords: 
				df_kw[counter] = glove_model[keyword]
				counter+=1

			kw_km = KMeans(n_clusters=1).fit(df_kw.T) 
			kw_v = kw_km.cluster_centers_[0] 
			selected_keyword = glove_model.most_similar(positive=[kw_v], topn=1)
			output['keyword_'+str(i)] = {
			'keywords': keywords,
			'selected_keyword': selected_keyword[0][0]
			}

		#output = json.dumps(output, indent = 4)

		return output

	except Exception as E:
			return E

import gensim.downloader as api
glove_model = api.load('fasttext-wiki-news-subwords-300')

def check_drive_mounted():
    drive_status = False
    try:
        with open('/content/gdrive/My Drive/check_mount.txt', 'w') as file:
            file.write('Mount test')
        drive_status = True
    except:
        drive_status = False
    finally:
        if drive_status:
            #print("Google Drive is already mounted.")
            return True
        else:
            #print("Google Drive is not mounted.")
            return False

def read_text_file(file_path):
  if not check_drive_mounted(): drive.mount('/content/gdrive')
  try:
    with open(file_path, 'r') as file:
      return file.read()
  except Exception as E:
    return E

file_path = "/content/gdrive/MyDrive/zyte_output.txt"
text = read_text_file(file_path)

file_path = "/content/gdrive/MyDrive/zyte_output_news.txt"
text_n = read_text_file(file_path)

t = kmeans_rmodel(text, glove_model, 30)

t_n = kmeans_rmodel(text_n, glove_model, 30)

def kmm(k=15):
  print(k)

l1 = [t_n[v]['selected_keyword'] for v in t_n]
l2 = [t[v]['selected_keyword'] for v in t]

t